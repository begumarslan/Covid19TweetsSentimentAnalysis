{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8937bbc3",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e10b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag \n",
    "import spacy\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from textblob import Word\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c09e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the twitter data\n",
    "covid_tweets = pd.read_csv('covid_tweets_2021.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b68c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing html code at the end of multiple rows\n",
    "covid_tweets['Text'] = covid_tweets['Text'].str.split('https',expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4016d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing spaces, punctuations, lowering letters , low frequency words\n",
    "\n",
    "def preprocessing(df, column):\n",
    "    df[column] = df[column].apply(lambda x: x.lower())           \n",
    "    df[column] = df[column].apply(lambda x: re.sub(r\"\\W\",\" \", x))\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r\"\\d\",\"\", x))\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r\"^\\s\",\"\", x))\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r\"\\s$\",\"\", x))\n",
    "    emojis = list(EMOTICONS_EMO.keys())\n",
    "    df[column] = df[column].apply(lambda x: ''.join([c for c in x if c not in emojis]))\n",
    "    df[column] = df[column].apply(lambda x: ''.join([c for c in x if c not in string.punctuation]))\n",
    "    low_frequency_words = pd.Series(' '.join(df[column]).split()).value_counts()[-1000:]\n",
    "    df[column] = df[column].apply(lambda x: \" \".join(x.lower() for x in x.split()if x not in low_frequency_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652e3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(covid_tweets,column='Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d6ac17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removing stopwords with nltk\n",
    "english_stopwords = stopwords.words('english')\n",
    "covid_tweets[\"Text\"]=covid_tweets[\"Text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34d660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords with spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "sp_stopwords = sp.Defaults.stop_words\n",
    "covid_tweets[\"Text\"]=covid_tweets[\"Text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sp_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb04021",
   "metadata": {},
   "source": [
    "### Adjectives with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855a5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column for adjectives in the tweet\n",
    "\n",
    "def getAdjectives(tweet):\n",
    "    tweet = word_tokenize(tweet) \n",
    "    #JJ : stands for adjectives\n",
    "    tweet = [word for (word, tag) in pos_tag(tweet) if tag == \"JJ\"]  \n",
    "    return \" \".join(tweet)  \n",
    "\n",
    "covid_tweets['Adjectives']=covid_tweets['Text'].apply(lambda x: getAdjectives(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbdf81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation \n",
    "covid_tweets['Text']=covid_tweets[\"Text\"].apply(lambda x: \" \".join([Word(i).lemmatize() for i in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb28f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-31 23:59:58+00:00</td>\n",
       "      <td>1377410374551568387</td>\n",
       "      <td>new study ontario make wonder younger child k ...</td>\n",
       "      <td>freespiritus</td>\n",
       "      <td>new wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-31 23:59:56+00:00</td>\n",
       "      <td>1377410366032932865</td>\n",
       "      <td>johnson amp johnson covid vaccine dos delayed ...</td>\n",
       "      <td>paldhous</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31 23:59:55+00:00</td>\n",
       "      <td>1377410361360482304</td>\n",
       "      <td>ontario icu overwhelmed record high occupancy ...</td>\n",
       "      <td>StevenDelDuca</td>\n",
       "      <td>high red grave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-31 23:59:53+00:00</td>\n",
       "      <td>1377410351633940484</td>\n",
       "      <td>american getting high speed internet new biden...</td>\n",
       "      <td>hardknoxfirst</td>\n",
       "      <td>american high new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-31 23:59:50+00:00</td>\n",
       "      <td>1377410338572759041</td>\n",
       "      <td>knew case rising france going lockdown smh par...</td>\n",
       "      <td>SpencerKarter</td>\n",
       "      <td>lockdown smh covid lockdown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime             Tweet Id  \\\n",
       "0  2021-03-31 23:59:58+00:00  1377410374551568387   \n",
       "1  2021-03-31 23:59:56+00:00  1377410366032932865   \n",
       "2  2021-03-31 23:59:55+00:00  1377410361360482304   \n",
       "3  2021-03-31 23:59:53+00:00  1377410351633940484   \n",
       "4  2021-03-31 23:59:50+00:00  1377410338572759041   \n",
       "\n",
       "                                                Text       Username  \\\n",
       "0  new study ontario make wonder younger child k ...   freespiritus   \n",
       "1  johnson amp johnson covid vaccine dos delayed ...       paldhous   \n",
       "2  ontario icu overwhelmed record high occupancy ...  StevenDelDuca   \n",
       "3  american getting high speed internet new biden...  hardknoxfirst   \n",
       "4  knew case rising france going lockdown smh par...  SpencerKarter   \n",
       "\n",
       "                    Adjectives  \n",
       "0                     new wear  \n",
       "1                      delayed  \n",
       "2               high red grave  \n",
       "3            american high new  \n",
       "4  lockdown smh covid lockdown  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9bee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tweets.to_csv('preprocessed_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
